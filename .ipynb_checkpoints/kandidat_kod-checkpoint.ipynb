{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bed436-6326-4dad-811a-fc616d6b993b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83d060-1128-47ee-839b-9a4923f1b803",
   "metadata": {},
   "source": [
    "### python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc3cd28-bfd6-4dea-93fa-5cd5dc794aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # for importing and working with .csv files\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import scipy.fft as spfft # for functions such as fft\n",
    "import scipy.signal as spsig # for signal processing (filtering)\n",
    "import tensorflow as tf # for machine learning using tensorflow keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589cb43-5a82-4233-a84f-00cfd76824a0",
   "metadata": {},
   "source": [
    "### data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782fee4b-a2e6-4d70-a9c1-80041f06c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading given csv file  \n",
    "# and creating dataframe \n",
    "log_df = pd.read_csv(\"serial_log1.txt\", header = None, delimiter=' ') \n",
    "  \n",
    "# adding column headings \n",
    "log_df.columns = ['Timestamp', 'Name', 'Distance', 'Unit'] \n",
    "\n",
    "#log_df.drop('Name', axis=1)\n",
    "#log_df.drop('Unit', axis=1)\n",
    "\n",
    "log_df = log_df.drop(columns=['Name', 'Unit'])\n",
    "\n",
    "# store dataframe into csv file \n",
    "log_df.to_csv('Log1.csv',  \n",
    "                index = None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1211fa-3c8b-40ba-a497-104927a31fbc",
   "metadata": {},
   "source": [
    "## Fourier transform on signals\n",
    "We preform the fourier transform in order to see what frequencies we might want to filter out.\n",
    "https://docs.scipy.org/doc/scipy/tutorial/fft.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20b349-bd12-4bd6-9c14-d2e5cf9328df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preforming the fourier transform and plotting using fft from scipy and plottig with plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c2ecc-9cec-489b-9dfa-97123e95b3b9",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "Using filters a lot of the noise from the different signals can be removed.\n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7605b-01bf-4c23-a9ed-3741f53f47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal filtering using scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28cd44-81f5-4490-8c8d-5ff201568d73",
   "metadata": {},
   "source": [
    "## other pre-precessing\n",
    "I.E splitting the data into cycles of skiing and extracting the variables we want to input into the machine learning algorithm (How close the feet are togheter when the maximum pressure is reached, among others...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ff786-1f80-4d8f-a43a-b6dffe4d9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into cycles using some threshhold for the data (ex perhaps the switch of acceleration direction of one of the boots)\n",
    "\n",
    "# extracting the variables we want to look at in the data (see rapport) and input into matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70df8e-1b10-4df2-b301-1fd922888293",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c011cb-3c0a-4b3b-8bb8-29814efc956b",
   "metadata": {},
   "source": [
    "### train-test split\n",
    "Splitting the data into a training set and a test set (validation)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e5176-a633-4699-809a-793d2f0d4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\"\"\"\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=random_seed)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebaf39-5a5f-4bb1-8faf-c82145704fda",
   "metadata": {},
   "source": [
    "### constructing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2238aab-81ef-407c-8ba2-09b4c6c80616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu,  \n",
    "                                activity_regularizer=tf.keras.regularizers.L2(1e-5))) # change 128 based on amount of input vaiables\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu,\n",
    "                                activity_regularizer=tf.keras.regularizers.L2(1e-5))) # play around whith size to see what works (overfit->smaller, underfit->larger(or add another layer))\n",
    "model.add(tf.keras.layers.Dense(1,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']) # could perhaps add loss here aswell\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb57f20-68b3-4dd6-958a-8dba42becf83",
   "metadata": {},
   "source": [
    "### training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e079d-f647-4ebb-9b24-9db2f8232b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(x_train,y_train,epochs=3) # probably add more epochs when we've found something that works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9b06d-8e94-496e-9d3f-d45e8311dfb0",
   "metadata": {},
   "source": [
    "### validating model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763c348-b6e9-441c-a150-2dbf2cffba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss, val_accuracy = model.evaluate(x_test,y_test)\n",
    "#print(val_loss, val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
